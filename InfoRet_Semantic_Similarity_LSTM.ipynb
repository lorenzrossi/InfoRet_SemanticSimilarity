{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk/l4ZR3r949N2vsyk/SHM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzrossi/InfoRet_SemanticSimilarity/blob/main/InfoRet_Semantic_Similarity_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-MVUr8bYRjvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d100d3-63c8-49f5-e274-8a2156827991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=b58fdb4ffbad5dd7ca61cdd6fff89bf0eb6956ce1b4f4f216338aa7204c74aae\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (7.0.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.25.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (4.0.0)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ],
      "source": [
        "# Generics\n",
        "\n",
        "!pip install wget\n",
        "!pip install opendatasets\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import shutil, sys, tqdm, re, spacy, string, argparse, os, pickle, pprint, gc, io, pdb, zipfile, wget, h5py, subprocess\n",
        "\n",
        "from datetime import datetime\n",
        "import opendatasets as op\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from string import punctuation\n",
        "\n",
        "# metrics \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "# PYTORCH\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "from torchtext import datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence \n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, random_split, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# NLTK\n",
        "import nltk as nlp\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "nlp.download('punkt')\n",
        "nlp.download('wordnet')\n",
        "nlp.download('stopwords')\n",
        "nlp.download('popular')\n",
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(glove='glove.6B.zip'):\n",
        "    print(\"Downloading word embedding\")\n",
        "    downloaded_glove1 = wget.download(\"http://nlp.stanford.edu/data/{}\".format('glove.6B.zip'))\n",
        "    \n",
        "    if not os.path.exists(\"./data\"):\n",
        "        os.mkdir(\"./data\")\n",
        "    print(\"Extracting\")\n",
        "    zip = zipfile.ZipFile(downloaded_glove1)\n",
        "    zip.extractall(path=\"./data\")\n",
        "    print(\"done!\")"
      ],
      "metadata": {
        "id": "lS4SVHqz5LdB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIj620icHYBQ",
        "outputId": "cd2abf03-ec22-4caa-a388-27808fa4da56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading word embedding\n",
            "Extracting\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "45tor9Q8_yEd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path = './data/glove.6B.300d.txt'"
      ],
      "metadata": {
        "id": "OB6414gvYQ1K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "op.download(\"https://www.kaggle.com/datasets/stanfordu/stanford-natural-language-inference-corpus\")\n",
        "\n",
        "dataset_folder = 'stanford-natural-language-inference-corpus'\n",
        "\n",
        "# bcc30972b8b9f25c2bc6c0a46d8f4d62"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE_TYL8E_8YE",
        "outputId": "3ca31686-36d1-40e3-fcd6-ec534d0938bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: lorenzrossi\n",
            "Your Kaggle Key: ··········\n",
            "Downloading stanford-natural-language-inference-corpus.zip to ./stanford-natural-language-inference-corpus\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.4M/44.4M [00:00<00:00, 90.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(os.path.join(dataset_folder, \"snli_1.0_train.csv\"))\n",
        "val_df = pd.read_csv(os.path.join(dataset_folder, \"snli_1.0_dev.csv\"))\n",
        "\n",
        "train_df['sentence1'] = train_df['sentence1'].astype(str)\n",
        "train_df['sentence2'] = train_df['sentence2'].astype(str)\n",
        "\n",
        "val_df['sentence1'] = val_df['sentence1'].astype(str)\n",
        "val_df['sentence2'] = val_df['sentence2'].astype(str)\n",
        "\n",
        "train_df = train_df[(train_df['sentence1'].str.split().str.len() > 0) & (train_df['sentence2'].str.split().str.len() > 0)]\n",
        "val_df = val_df[(val_df['sentence1'].str.split().str.len() > 0) & (val_df['sentence2'].str.split().str.len() > 0)]\n",
        "\n",
        "train_df = train_df[['gold_label', 'sentence1', 'sentence2']][train_df['gold_label'] != '-']\n",
        "val_df = val_df[['gold_label', 'sentence1', 'sentence2']][val_df['gold_label'] != '-']"
      ],
      "metadata": {
        "id": "k6o-YtbtJRdd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_map(count):  \n",
        "  word_map = {}\n",
        "  for num in count:\n",
        "    if num in word_map:\n",
        "      word_map[num] += 1\n",
        "    else:\n",
        "      word_map[num] = 1\n",
        "\n",
        "  return word_map"
      ],
      "metadata": {
        "id": "pVW4u2A4ubvb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_premise = train_df['sentence1'].str.split().str.len()\n",
        "get_word_map(count_premise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cKqIb3jKBNK",
        "outputId": "60612e6f-4f1b-4ae8-c0c9-20a90dbbe988"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{11: 47950,\n",
              " 6: 25521,\n",
              " 13: 37865,\n",
              " 26: 3338,\n",
              " 7: 36562,\n",
              " 14: 32913,\n",
              " 10: 50609,\n",
              " 15: 26970,\n",
              " 17: 19876,\n",
              " 23: 6481,\n",
              " 16: 22773,\n",
              " 22: 8132,\n",
              " 35: 510,\n",
              " 18: 18052,\n",
              " 9: 49129,\n",
              " 12: 43021,\n",
              " 8: 43177,\n",
              " 5: 10811,\n",
              " 19: 15722,\n",
              " 21: 10004,\n",
              " 42: 135,\n",
              " 24: 5250,\n",
              " 33: 786,\n",
              " 20: 12986,\n",
              " 4: 3314,\n",
              " 25: 4218,\n",
              " 30: 1259,\n",
              " 31: 1208,\n",
              " 36: 384,\n",
              " 37: 328,\n",
              " 27: 2528,\n",
              " 3: 639,\n",
              " 29: 1938,\n",
              " 28: 1946,\n",
              " 51: 33,\n",
              " 32: 797,\n",
              " 38: 322,\n",
              " 2: 63,\n",
              " 39: 207,\n",
              " 50: 51,\n",
              " 34: 723,\n",
              " 40: 170,\n",
              " 43: 60,\n",
              " 53: 15,\n",
              " 48: 42,\n",
              " 49: 48,\n",
              " 41: 123,\n",
              " 55: 15,\n",
              " 78: 15,\n",
              " 45: 24,\n",
              " 44: 99,\n",
              " 54: 9,\n",
              " 62: 6,\n",
              " 46: 57,\n",
              " 61: 3,\n",
              " 63: 6,\n",
              " 57: 3,\n",
              " 64: 30,\n",
              " 47: 48,\n",
              " 59: 3,\n",
              " 70: 3,\n",
              " 52: 3,\n",
              " 60: 6,\n",
              " 56: 3,\n",
              " 58: 3,\n",
              " 68: 15,\n",
              " 65: 6,\n",
              " 69: 15,\n",
              " 72: 3,\n",
              " 73: 3}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_hypothesis = train_df['sentence2'].str.split().str.len()\n",
        "get_word_map(count_hypothesis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJkoZsoAKHgo",
        "outputId": "5b1a606c-1e2c-4446-f893-5cce35191e9c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{9: 46982,\n",
              " 7: 88656,\n",
              " 6: 93996,\n",
              " 4: 62090,\n",
              " 16: 3637,\n",
              " 5: 70582,\n",
              " 8: 61890,\n",
              " 12: 15940,\n",
              " 14: 7374,\n",
              " 17: 2448,\n",
              " 11: 22992,\n",
              " 20: 839,\n",
              " 19: 1209,\n",
              " 10: 33588,\n",
              " 2: 1609,\n",
              " 13: 10868,\n",
              " 18: 1748,\n",
              " 3: 15735,\n",
              " 21: 565,\n",
              " 15: 5184,\n",
              " 23: 288,\n",
              " 24: 173,\n",
              " 27: 66,\n",
              " 33: 14,\n",
              " 25: 150,\n",
              " 31: 22,\n",
              " 22: 398,\n",
              " 30: 28,\n",
              " 32: 12,\n",
              " 35: 10,\n",
              " 28: 59,\n",
              " 26: 103,\n",
              " 29: 38,\n",
              " 38: 6,\n",
              " 1: 42,\n",
              " 43: 2,\n",
              " 34: 9,\n",
              " 40: 3,\n",
              " 48: 1,\n",
              " 36: 4,\n",
              " 56: 1,\n",
              " 37: 2,\n",
              " 50: 1,\n",
              " 46: 1,\n",
              " 55: 1,\n",
              " 39: 1}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, stop_words = False, lemmatization = False):\n",
        "  text = text.lower().split()\n",
        "  if stop_words:\n",
        "    stop = stopwords.words('english')\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops]\n",
        "  \n",
        "  text = \" \".join(text)\n",
        "  text = re.sub(\"[^A-Za-z']+\", ' ', str(text)).replace(\"'\", '')\n",
        "  text = re.sub(r\"\\bum*\\b\", \"\", text)\n",
        "  text = re.sub(r\"\\buh*\\b\", \"\", text)\n",
        "  text = re.sub(r\"won\\'t\", \"will not\", text)\n",
        "  text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "  text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "  text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "  text = re.sub(r\"\\'re\", \" are\", text)\n",
        "  text = re.sub(r\"\\'s\", \" is\", text)\n",
        "  text = re.sub(r\"\\'d\", \" would\", text)\n",
        "  text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "  text = re.sub(r\"\\'t\", \" not\", text)\n",
        "  text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "  text = re.sub(r\"\\'m\", \" am\", text)\n",
        "  if lemmatization:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(text)])\n",
        "  \n",
        "  text = text.translate(str.maketrans('', '', punctuation))\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "XLKYgTEYudWG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['sentence1'] = train_df['sentence1'].astype(str).apply(lambda text: clean_text(text))\n",
        "train_df['sentence2'] = train_df['sentence2'].astype(str).apply(lambda text: clean_text(text))\n",
        "val_df['sentence1'] = val_df['sentence1'].astype(str).apply(lambda text: clean_text(text))\n",
        "val_df['sentence2'] = val_df['sentence2'].astype(str).apply(lambda text: clean_text(text))"
      ],
      "metadata": {
        "id": "b84Ms9yVL-LX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[(train_df['sentence1'].str.split().str.len() > 0) & (train_df['sentence2'].str.split().str.len() > 0)]\n",
        "val_df = val_df[(val_df['sentence1'].str.split().str.len() > 0) & (val_df['sentence2'].str.split().str.len() > 0)]"
      ],
      "metadata": {
        "id": "KIzG8oqfsIvZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([train_df, val_df])"
      ],
      "metadata": {
        "id": "29m7QhZAMFrN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization and Vocabulary"
      ],
      "metadata": {
        "id": "PNJjFMVLDwVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pair_generator(df):\n",
        "  sentence_pair  = []\n",
        "  sentence_label = []\n",
        "  for _, row in df.iterrows():\n",
        "    sentence_pair.append((row['sentence1'], row['sentence2']))\n",
        "    sentence_label.append(row['gold_label'])\n",
        "  return sentence_pair, sentence_label"
      ],
      "metadata": {
        "id": "7R2LCeuXMHTh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_pairs, _ = pair_generator(df)\n",
        "train_sentence_pairs, train_sentence_labels = pair_generator(train_df)\n",
        "val_sentence_pairs, val_sentence_labels = pair_generator(val_df)"
      ],
      "metadata": {
        "id": "HgGyGhfkMP59"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = set(train_sentence_labels)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_CL4YKgMZ6z",
        "outputId": "fd141af9-a1d1-4cdb-9209-0ac15890e66e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'contradiction', 'entailment', 'neutral'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag2idx = {word: i for i, word in enumerate(labels)}\n",
        "print(tag2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L5RkReDMchd",
        "outputId": "39fcce9d-8cee-4100-cfca-7a84785b6fbc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'contradiction': 0, 'entailment': 1, 'neutral': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = [tag2idx[t] for t in train_sentence_labels]\n",
        "val_labels = [tag2idx[t] for t in val_sentence_labels]"
      ],
      "metadata": {
        "id": "PW77ZxuyMfaj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {}\n",
        "    self.n_words = 0\n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    for word in word_tokenize(sentence):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words + 1\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words + 1] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "94sgaOqRMjUS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSetLoader(Dataset):\n",
        "  def __init__(self, sentence_pair, labels):\n",
        "    self.sentence_pair = sentence_pair\n",
        "    self.labels        = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentence_pair)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.sentence_pair[index], self.labels[index]"
      ],
      "metadata": {
        "id": "-xqqxEckMkOu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocabulary()\n",
        "for data in [sentence_pairs]:\n",
        "  for sentence_pair in data:\n",
        "    premise = sentence_pair[0]\n",
        "    hypothesis = sentence_pair[1]\n",
        "    vocab.addSentence(premise)\n",
        "    vocab.addSentence(hypothesis)\n",
        "\n",
        "print(\"Vocab size:\", len(vocab.word2index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrGye183MsOf",
        "outputId": "ba9cecb9-9b3b-45d5-f313-fd0d1c8ce961"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 33353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pair_indices(sentence_pairs):\n",
        "  indices_pairs = []\n",
        "  for sentence_pair in sentence_pairs:\n",
        "    premise = sentence_pair[0]\n",
        "    premise_indices = [vocab.word2index[w] for w in word_tokenize(premise)]\n",
        "    hypothesis = sentence_pair[1]\n",
        "    hypothesis_indices = [vocab.word2index[w] for w in word_tokenize(hypothesis)]\n",
        "    indices_pairs.append((premise_indices, hypothesis_indices))\n",
        "  return indices_pairs"
      ],
      "metadata": {
        "id": "1eEk1l8GM4Ty"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = DataSetLoader(get_pair_indices(train_sentence_pairs), train_labels)\n",
        "val_data = DataSetLoader(get_pair_indices(val_sentence_pairs), val_labels)"
      ],
      "metadata": {
        "id": "aQC0yF9aM7AR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding"
      ],
      "metadata": {
        "id": "QR9OnoDJNk4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_SIZE = 300\n",
        "VOCAB_SIZE = len(vocab.word2index)\n",
        "TARGET_SIZE = len(tag2idx)\n",
        "HIDDEN_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "STACKED_LAYERS = 2"
      ],
      "metadata": {
        "id": "tS3JE4nEM_Y1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = BATCH_SIZE, collate_fn=lambda x:x)\n",
        "val_loader   = torch.utils.data.DataLoader(val_data, batch_size = BATCH_SIZE, collate_fn=lambda x:x)\n",
        "\n",
        "print(len(train_loader), len(val_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-7t8GGSNjgX",
        "outputId": "e98b1b54-a2ac-4d72-a82c-86970bb06616"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17168 308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_coefs(word, *arr):\n",
        "  return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "def load_embeddings(path):\n",
        "  with open(path) as f:\n",
        "    return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
        "\n",
        "embeddings_index = load_embeddings(glove_path)"
      ],
      "metadata": {
        "id": "_hjrTpIoN5L5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = 1 * np.random.randn(VOCAB_SIZE + 1, EMBEDDING_SIZE)\n",
        "embedded_count = 0\n",
        "for word, lang_word_index in vocab.word2index.items():\n",
        "  if embeddings_index.get(word) is not None:\n",
        "    weights[lang_word_index] = embeddings_index.get(word)\n",
        "    embedded_count += 1\n",
        "\n",
        "print(\"Embedded count:\", embedded_count)\n",
        "del embeddings_index\n",
        "weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbBSgWg7OMnF",
        "outputId": "6cbbc733-ebf9-477c-b821-848409e6e965"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded count: 27188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33354, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "RdGjbB0wT1op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3lNkimsOT3Y",
        "outputId": "3825b10d-c700-46ad-9ab5-afa2ec3a1323"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.tokenize import word_tokenize \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import seaborn as sns\n",
        "from string import punctuation\n",
        "import re\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QRq2kvFO_UY",
        "outputId": "082c0e50-4bb6-4eef-ef10-c427032919da"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, target_size, stacked_layers, weights_matrix, bidirectional):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bidirectional = bidirectional\n",
        "    self.target_size = target_size\n",
        "    self.stacked_layers = stacked_layers\n",
        "    num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
        "    self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "    self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "    self.embedding.weight.requires_grad = True\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size, num_layers=self.stacked_layers, \n",
        "                        batch_first = True, dropout=0.2, bidirectional=bidirectional)\n",
        "    \n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p = 0.2)\n",
        "\n",
        "    self.FC_concat1 = nn.Linear(2 * 2 * hidden_size if bidirectional else 2 * hidden_size, 128)\n",
        "    self.FC_concat2 = nn.Linear(128, 64)\n",
        "    self.FC_concat3 = nn.Linear(64, 32)\n",
        "\n",
        "    for lin in [self.FC_concat1, self.FC_concat2]:\n",
        "      nn.init.xavier_uniform_(lin.weight)\n",
        "      nn.init.zeros_(lin.bias)\n",
        "\n",
        "    self.output = nn.Linear(32, self.target_size)\n",
        "\n",
        "    self.out = nn.Sequential(\n",
        "\t\t\tself.FC_concat1,\n",
        "\t\t\tself.relu,\n",
        "\t\t\tself.dropout,\n",
        "\t\t\tself.FC_concat2,\n",
        "\t\t\tself.relu,\n",
        "      self.FC_concat3,\n",
        "      self.relu,\n",
        "\t\t\tself.dropout,\n",
        "\t\t\tself.output\n",
        "\t\t)\n",
        "\n",
        "  def forward_once(self, seq, hidden, seq_len):\n",
        "    embedd_seq = self.embedding(seq)\n",
        "    packed_seq = pack_padded_sequence(embedd_seq, lengths=seq_len, batch_first=True, enforce_sorted=False)\n",
        "    output, (hidden, _) = self.lstm(packed_seq, hidden)\n",
        "    return hidden\n",
        "\n",
        "  def forward(self, input, premise_len, hypothesis_len):\n",
        "    premise    = input[0]\n",
        "    hypothesis = input[1]\n",
        "    batch_size = premise.size(0)\n",
        "\n",
        "    h0 = torch.zeros(self.stacked_layers*2 if self.bidirectional else self.stacked_layers, batch_size, self.hidden_size).to(device) # 2 for bidirection \n",
        "    c0 = torch.zeros(self.stacked_layers*2 if self.bidirectional else self.stacked_layers, batch_size, self.hidden_size).to(device)\n",
        "\n",
        "    # hidden = self.init_hidden(batch_size)\n",
        "\n",
        "    premise    = self.forward_once(premise, (h0, c0), premise_len)\n",
        "    hypothesis = self.forward_once(hypothesis, (h0, c0), hypothesis_len)\n",
        "    \n",
        "    combined_outputs  = torch.cat((premise, hypothesis, torch.abs(premise - hypothesis), premise * hypothesis), dim=2)\n",
        "\n",
        "    return self.out(combined_outputs[-1])"
      ],
      "metadata": {
        "id": "8Nte6mYcOnMK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
        "  return acc"
      ],
      "metadata": {
        "id": "d4HyiNlWP4iJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, criterion, optimizer):  \n",
        "  total_step = len(train_loader)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_train_acc  = 0\n",
        "    for val in train_loader:\n",
        "      sentence_pairs, labels = map(list, zip(*val))\n",
        "\n",
        "      premise_seq    = [torch.tensor(seq[0]).long().to(device) for seq in sentence_pairs]\n",
        "      hypothesis_seq = [torch.tensor(seq[1]).long().to(device) for seq in sentence_pairs]\n",
        "      batch = len(premise_seq)\n",
        "\n",
        "      premise_len    = list(map(len, premise_seq))\n",
        "      hypothesis_len = list(map(len, hypothesis_seq))\n",
        "\n",
        "      temp = pad_sequence(premise_seq + hypothesis_seq, batch_first=True)\n",
        "      premise_seq    = temp[:batch, :]\n",
        "      hypothesis_seq = temp[batch:, :]\n",
        "      labels         = torch.tensor(labels).long().to(device)\n",
        "\n",
        "      model.zero_grad()\n",
        "      prediction = model([premise_seq, hypothesis_seq], premise_len, hypothesis_len)\n",
        "\n",
        "      loss = criterion(prediction, labels)\n",
        "      acc  = multi_acc(prediction, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      total_train_loss += loss.item()\n",
        "      total_train_acc  += acc.item()\n",
        "\n",
        "    train_acc  = total_train_acc/len(train_loader)\n",
        "    train_loss = total_train_loss/len(train_loader)\n",
        "    model.eval()\n",
        "    total_val_acc  = 0\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for val in val_loader:\n",
        "        sentence_pairs, labels = map(list, zip(*val))\n",
        "\n",
        "        premise_seq    = [torch.tensor(seq[0]).long().to(device) for seq in sentence_pairs]\n",
        "        hypothesis_seq = [torch.tensor(seq[1]).long().to(device) for seq in sentence_pairs]\n",
        "        batch = len(premise_seq)\n",
        "\n",
        "        premise_len    = list(map(len, premise_seq))\n",
        "        hypothesis_len = list(map(len, hypothesis_seq))\n",
        "\n",
        "        temp = pad_sequence(premise_seq + hypothesis_seq, batch_first=True)\n",
        "        premise_seq    = temp[:batch, :]\n",
        "        hypothesis_seq = temp[batch:, :]\n",
        "\n",
        "        premise_seq    = premise_seq.to(device)\n",
        "        hypothesis_seq = hypothesis_seq.to(device)\n",
        "        labels         = torch.tensor(labels).long().to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        prediction = model([premise_seq, hypothesis_seq], premise_len, hypothesis_len)\n",
        "        \n",
        "        loss = criterion(prediction, labels)\n",
        "        acc  = multi_acc(prediction, labels)\n",
        "\n",
        "        total_val_loss += loss.item()\n",
        "        total_val_acc  += acc.item()\n",
        "\n",
        "    val_acc  = total_val_acc/len(val_loader)\n",
        "    val_loss = total_val_loss/len(val_loader)\n",
        "\n",
        "    end = time.time()\n",
        "    hours, rem = divmod(end-start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
        "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "b52IuJwwO1xU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = LSTM(vocab_size = VOCAB_SIZE, \n",
        "                  hidden_size = HIDDEN_SIZE, \n",
        "                  target_size = TARGET_SIZE, \n",
        "                  stacked_layers = STACKED_LAYERS, \n",
        "                  weights_matrix = weights, \n",
        "                  bidirectional = True)\n",
        "lstm_model.to(device)\n",
        "print(lstm_model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0wbIi_8QIau",
        "outputId": "a3a07c24-8f9a-461d-8356-99cb89afa103"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (embedding): Embedding(33354, 300)\n",
            "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (FC_concat1): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (FC_concat2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (FC_concat3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (output): Linear(in_features=32, out_features=3, bias=True)\n",
            "  (out): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.2, inplace=False)\n",
            "    (8): Linear(in_features=32, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(lstm_model, train_loader, val_loader, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFT653-XQybz",
        "outputId": "88667e17-1376-4532-b76e-ee0bc49e3d71"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00:04:18.42\n",
            "Epoch 1: train_loss: 0.6911 train_acc: 0.7096 | val_loss: 0.5801 val_acc: 0.7603\n",
            "00:04:18.42\n",
            "Epoch 2: train_loss: 0.5851 train_acc: 0.7651 | val_loss: 0.5226 val_acc: 0.7885\n",
            "00:04:18.42\n",
            "Epoch 3: train_loss: 0.5486 train_acc: 0.7828 | val_loss: 0.5075 val_acc: 0.8002\n",
            "00:04:18.89\n",
            "Epoch 4: train_loss: 0.5256 train_acc: 0.7936 | val_loss: 0.5006 val_acc: 0.7991\n",
            "00:04:17.69\n",
            "Epoch 5: train_loss: 0.5084 train_acc: 0.8014 | val_loss: 0.4852 val_acc: 0.8110\n",
            "00:04:17.79\n",
            "Epoch 6: train_loss: 0.4954 train_acc: 0.8075 | val_loss: 0.4808 val_acc: 0.8087\n",
            "00:04:17.94\n",
            "Epoch 7: train_loss: 0.4850 train_acc: 0.8121 | val_loss: 0.4733 val_acc: 0.8118\n",
            "00:04:18.56\n",
            "Epoch 8: train_loss: 0.4771 train_acc: 0.8158 | val_loss: 0.4730 val_acc: 0.8131\n",
            "00:04:19.60\n",
            "Epoch 9: train_loss: 0.4680 train_acc: 0.8203 | val_loss: 0.4777 val_acc: 0.8161\n",
            "00:04:17.81\n",
            "Epoch 10: train_loss: 0.4623 train_acc: 0.8223 | val_loss: 0.4770 val_acc: 0.8157\n"
          ]
        }
      ]
    }
  ]
}